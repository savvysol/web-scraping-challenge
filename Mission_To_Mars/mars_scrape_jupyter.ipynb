{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://splinter.readthedocs.io/en/latest/drivers/chrome.html\n",
    "#!which chromedriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs to Scrape\n",
    "news_url = 'https://mars.nasa.gov/news'\n",
    "image_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "weather_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "facts_url = 'https://space-facts.com/mars/'\n",
    "hemi_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it easy to pass in a URL to lauch.\n",
    "# Requires 'http://'\n",
    "def launch(url):\n",
    "    executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "    browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that opens a browser and grabs the soup\n",
    "\n",
    "def scrape_url(url):\n",
    "    # Set the Chrome Path and Launch the Browser\n",
    "    executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "    \n",
    "    # Pass in a a URL to visit\n",
    "    browser.visit(url)\n",
    "    \n",
    "    #give it a moment to load\n",
    "    time.sleep(4)\n",
    "    \n",
    "    # Grab the HTML & Soup it!\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    browser.quit()\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in the news url and return a list of dictionaries for the results\n",
    "\n",
    "def get_news(url):\n",
    "    \n",
    "    # scrape the new url\n",
    "    soup = scrape_url(url)\n",
    "    \n",
    "    # Get the News Slides from the Soup & Set up a Dictionary\n",
    "    slides = soup.find_all('li',class_='slide')\n",
    "    mars_news = []\n",
    "    mars_dict = {}\n",
    "\n",
    "    # Loop Through the Slides and Grap the Title and Teaser\n",
    "    for x in range(len(slides)):\n",
    "        title = slides[x].find('div', class_=\"content_title\").text.replace(\"/n\",\"\")\n",
    "        teaser = slides[x].find('div', class_=\"article_teaser_body\").text.replace(\"/n\",\"\")\n",
    "\n",
    "        # Add key, value pairs to dictionary\n",
    "        post = {\n",
    "            \"title\":title,\n",
    "            \"teaser\":teaser,\n",
    "        }\n",
    "\n",
    "        #Get the list of dictionaries\n",
    "        mars_news.append(post)\n",
    "\n",
    "    return mars_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in the Mars Image Url and return the image path\n",
    "def get_image(url):\n",
    "    # grap the soup\n",
    "    image_soup = scrape_url(url)\n",
    "    \n",
    "    #get the image path\n",
    "    image_path = image_soup.find_all('a', class_='button fancybox')[0]['data-fancybox-href']\n",
    "    full_image_path = f'https://www.jpl.nasa.gov{image_path}'\n",
    "    return full_image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in the weather url and return the text for the weather\n",
    "def get_weather(url):\n",
    "\n",
    "    soup = scrape_url(url)\n",
    "    \n",
    "    weather_full = soup.find_all('p', class_=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\")[0].text\n",
    "    weather = weather_full.split('pic.twitter')[0].replace('\\n',\"\")\n",
    "    return weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in the facts url and return a dataframe of the facts\n",
    "def get_facts(url):\n",
    "    tables = pd.read_html(url)\n",
    "    return tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in the Hemi url and return a list of urls to use for the next section\n",
    "def get_links(url):\n",
    "    base_url = 'https://astrogeology.usgs.gov'\n",
    "    soup = scrape_url(url)\n",
    "    items = soup.find_all(class_='item')\n",
    "    links = []\n",
    "    for i in items:\n",
    "        links.append(f\"{base_url}{i.find('a')['href']}\")\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_hemi(url):\n",
    "    # get list of urls from page\n",
    "    base_url = 'https://astrogeology.usgs.gov'\n",
    "    soup = scrape_url(url)\n",
    "    items = soup.find_all(class_='item')\n",
    "    links = []\n",
    "    for i in items:\n",
    "        links.append(f\"{base_url}{i.find('a')['href']}\")\n",
    "                     \n",
    "    # with each link, get titles & href\n",
    "    hemi = []\n",
    "    post = {}\n",
    "    for link in links:\n",
    "        soup = scrape_url(link)\n",
    "        title = soup.find('h2', class_='title').text\n",
    "        image_ref = soup.find_all('div', class_='downloads')[0].find_all('li')[0].a['href']\n",
    "\n",
    "        post = {\n",
    "            'title':title,\n",
    "            'hemi_image_ref':image_ref\n",
    "        }\n",
    "\n",
    "        hemi.append(post)\n",
    "\n",
    "    return hemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
